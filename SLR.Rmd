---
title: "SLR"
author: Maryann Zhao and Leyla Akay
output: word_document
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(message=FALSE, warning=FALSE, fig.height=3, fig.width=5, fig.align = "center")
library(dplyr)
library(ggplot2)
library(infer)
library(skimr)
library(broom)
library(mosaic)
library(knitr)
library(readr)
options(digits=3)
```

```{r, eval=TRUE, echo=FALSE}
tmdb_5000_movies <- read_csv("~/LM HW/SLM_Project/tmdb_5000_movies-OG.csv")
tmdb2 <- read_csv("tmdb_2.csv")

library(readr)
library(purrr)
df <- readr::read_csv("tmdb_5000_movies-OG.csv")
genres.table <- purrr::map(df$genres, jsonlite::fromJSON)
genres.table <- purrr::map_df(genres.table, ~data.frame(x=.x), .id="movieID")
```
##Introduction
We are studying a dataset of movies, that contains information about their languages, actors, genres, etc. For this project, our variables of interest are budget (in US Dollars) and popularity (based on number of votes per day, and number of views and favorites). We are interested in determining whether there is a linear relationship between the two, and if so, the nature of the relationship. Our null hypothesis is that there is no relationship between budget and popularity; our alternative hypothesis is that there is a positive relationship between budget and popularity. That is, $\beta_1 > 0$. 

To test this, first we must check that the data appears to fit with the technical assumptions necessary to perform a linear regression. The first is linearity. An initial plot of budget values vs. popularity indicates a linear shape. We transformed both explanatory and response variables with a log function, and noticed that the plot became more linear, so we decided to keep the transformation for the remaining tests. The second is independence--we are assuming mathematical independence. That is, spending more money doesn't necessarily translate to increased popularity, or vice versa. The final assumptions are normality and constant errors. To check this, we plotted the residuals and looked at the symmetry. The residuals appear to be symmetrically distributed around 0, with some increased scattering at lower values of budget. The residuals appear more concentrated around 0, and are all in the range of +/- 4.  

```{r, eval=TRUE}
#plot of log popularity vs. log budget
ggplot(tmdb2, aes(x=budget, y=popularity)) +
  geom_point() +
  scale_x_log10() +
  scale_y_log10()

#residual plot
bud2 <- tmdb2 %>% 
  select(popularity, budget) %>%
  filter(budget > 0, popularity > 0)
pop.log <- log(bud2$popularity)
bud.log <- log(bud2$budget)
movies <- lm(pop.log ~ bud.log, data = tmdb2)
ggplot (movies, aes(x=fitted(movies), y=resid(movies))) + geom_point()
```
##Testing $\beta_1$
To determine whether the alternative hypothesis holds, we found a 95% confidence interval for $\beta_1$. We found the CI was between 0.317 and 0.353. Because both variables had been logged-transformed, this means that a doubling of budget would be consistent with a multiplicative change in median of popularity between:
$2^(0.317) and 2^(0.353)$, which is consistent with 1.25 and 1.28. 

We were interested in movies with 1M$ budgets, since these are relatively low compared to many block-buster films, but many appear to achieve comparable levels of popularity. In this situation, we thought of budget as the explanatory variable, since it's determined before popularity, so although budget doesn't necessarily explain popularity, it wouldn't be possible for popularity to inform budget. 

```{r, eval=TRUE}
#CI and PI for 1 mill budget
broom::tidy(movies, conf.int = TRUE, conf.level = 0.95)
mill <- data.frame(bud.log=c(13.8))
crit_val <- qt(.95, glance(movies)$df.resid)
movie.pred <- broom::augment(movies, newdata=mill, type.predict = "response")
.se.pred <- sqrt(glance(movies)$sigma^2 + movie.pred$.se.fit)

movie.pred <- movie.pred %>%
mutate(lower_CI = .fitted - crit_val * .se.fit,
upper_CI = .fitted + crit_val * .se.fit)

movie.pred <- movie.pred %>%
  mutate(lower_PI = .fitted - crit_val * .se.pred,
         upper_PI = .fitted + crit_val * .se.pred, 
         lower_CI = .fitted - crit_val * .se.fit, 
         upper_CI = .fitted + crit_val * .se.fit)

movie.pred
```

##Assessing the fit
The r-squared value is 0.262. The residuals (plotted above) appear to be evenly distributed. Therefore, we believe our model accurately describes the data. 

```{r, eval=TRUE}
#R squared value
summary(movies)$r.squared
```

##Conclusion
In general, the positive relationship was expected. The decreased variability at very high budgets was interesting, but upon further reflection it makes sense. Movies that spend tens of millions of dollars probably attract high-profile actors and have lots of advertising campaigns, which probably increase the popularity. 

##Pairs: Simultaneous Inference
To find the mean and prediction intervals for all n values, we used the Bonferroni, Working-Hotelling, and Scheff√© methods. 

```{r, eval=TRUE}
#critical values for 3 methods
num_int <- 3
n <- 3766
crit_Bonf <- qt((1-.05)/num_int, glance(movies)$df.resid)
crit_WH <- sqrt(2*qf(.95, num_int, glance(movies)$df.resid))
crit_Sch <- sqrt(num_int*qf(.95, n-2, glance(movies)$df.resid))

#mean intervals for all n points
movie_gl <- broom::glance(movies)
movie_sig <- dplyr::pull(movie_gl, sigma)

movie_CI <-broom::augment(movies) %>%
  mutate(.se.pred = sqrt(movie_sig^2 + .se.fit^2)) %>%
  mutate(lower_CI = .fitted - crit_val * .se.fit,
  upper_CI = .fitted + crit_val * .se.fit, 
  lower_CI_B = .fitted - crit_Bonf*.se.fit,
  upper_CI_B = .fitted + crit_Bonf*.se.fit, 
  lower_CI_WH = .fitted - crit_WH*.se.fit,
  upper_CI_WH = .fitted + crit_WH*.se.fit,
  lower_CI_S = .fitted - crit_Sch*.se.fit,
  upper_CI_S = .fitted + crit_Sch*.se.fit)

#blue - no adjustment, red - Working-Hotelling, green - Bonferroni
ggplot(movie_CI, aes(x = bud.log, y =pop.log)) + geom_point() +
  stat_smooth(method = "lm", se = FALSE) +
  geom_ribbon(data=movie_CI, aes(ymin = lower_CI_B, ymax = upper_CI_B), alpha = .2, fill = "green") +
  geom_ribbon(data = movie_CI, aes(ymin = lower_CI_WH, ymax = upper_CI_WH), alpha = .2, fill = "red") +
  geom_ribbon(aes(ymin = lower_CI, ymax = upper_CI), alpha = .2, fill = "blue") +
  xlab("log(budget)") + ylab("log(popularity)")

#prediction interval for all n points
movie_PI <-broom::augment(movies) %>%
  mutate(.se.pred = sqrt(movie_sig^2 + .se.fit^2)) %>%
  mutate(lower_PI = .fitted - crit_val * .se.pred,
  upper_PI = .fitted + crit_val * .se.pred, 
  lower_PI_B = .fitted - crit_Bonf*.se.pred,
  upper_PI_B = .fitted + crit_Bonf*.se.pred, 
  lower_PI_WH = .fitted - crit_WH*.se.pred,
  upper_PI_WH = .fitted + crit_WH*.se.pred,
  lower_PI_S = .fitted - crit_Sch*.se.pred,
  upper_PI_S = .fitted + crit_Sch*.se.pred)

#blue - no adjustment, red - Scheffer, green - Bonferroni
ggplot(movie_PI, aes(x = bud.log, y =pop.log)) + geom_point() +
  stat_smooth(method = "lm", se = FALSE) +
  geom_ribbon(data=movie_PI, aes(ymin = lower_PI_B, ymax = upper_PI_B), alpha = .2, fill = "green") +
  geom_ribbon(data = movie_PI, aes(ymin = lower_PI_S, ymax = upper_PI_S), alpha = .2, fill = "red") +
  geom_ribbon(aes(ymin = lower_PI, ymax = upper_PI), alpha = .2, fill = "blue") +
  xlab("log(budget)") + ylab("log(popularity)")

```


It's important to control for multiple comparisons, because the more variables you compare, the more likely you are to get a "significant" difference, even though there might not be a real effect. For this reason, it's important to have stricter measures of confidence when testing multiple variables. For mean intervals, it's most useful to use the Working-Hotelling procedure, because it uses the entire range of x's, whereas the Bonferroni only uses the intervals of interest. However, for prediction intervals, Bonferroni is better than Scheffe because it takes into account the multiple comparisons error. 

